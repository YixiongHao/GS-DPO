# Documentation

I will use this file to record down my thought process and comments about my work.  
- I have not done DPO before, I first referenced (https://d2jud02ci9yv69.cloudfront.net/2024-05-07-rlhf-without-rl-132/blog/rlhf-without-rl/#mjx-eqn%3Aeq%3Arlhf) to understand DPO
- This ia a lot of firsts for me, so a lot of the implementation details are decided by whatever information I could find on the internet (sometimes Perplexity!).
- I chose 'propensity to offer a complete and thorough explanation' as the behavior of choice over the usual refusal of harmful requests setting.  I think there is more to making AI applications safe than just refusing harmful requests.  Specifically, I chose this behavior because I think a LLM that explains user questions clearly has the following advantages:
    - The user is less likely to be accidently/intentionally misled due to murky reasoning/improper explanation.
    - Make CoT monitoring & oversight easier.
    - Generally, make LLM assistants more useful


# Note on LLM generated code

Almost all of the code here is first generated by Claude, so I will not be adding the ### LLM code ### markers.  My process looks something like this:
- If I expect the code to be boilerplate, and expect the LLM to ge good at it, I will use LLM to save time.
- I then touch reality and debug.

# Part 0: Dataset

I chose https://huggingface.co/datasets/vincentmin/eli5_rlhf_explainlikeim5.  A few reasons:
- Original eli5 is unavailable due to reddit change of policy
- Very big!  Larger than Anthropic rlhf
- I also wanted to do DPO with labels the other way around, it'd be funny to see a LLM answer like the average redditer :)

# Part 1: Synthetic Evaluation Dataset

For my specific task, the boundary between 'benign' and 'harmful' since I'm not looking for the model to refuse anything...

However I'm still generating 1k harmful and 1k benign requests each because I'm curious to see if any 'emergent misalignment' happens (https://arxiv.org/pdf/2502.17424).  Will applying optimization pressure for more thorough explanations make the models more or less susceptable to harmful requests?

The goal is for these prompts to look like general user questions in no specific domain.

Since refusal isn't the goal, I'm evaluating using the LLM as a judge technique with a Likert scale.  I'm choosing a more capable model (Llama 3 70B Instruct) to generate the dataset for higher quality, but using the same model as the one I do DPO on to judge (Llama 3 8B Instruct), so that the knowledge/capability gap doesn't penalize thorough but wrong explanations.

Method:
1. Ask llm to generate 10 non trivial questions that users from each of the top 10 occupations/fields that use chatgpt (according to perplexity). 
    - For the harmful requests, we do this but with types of harmful requests.
2. Now we have 10x10 matrix of questions.  We perform prompt mutation to get different variations via prompting.

# Part 2: DPO

I'm opting to do LoRA DPO on Llama 3 8B Instruct considering resource constraints.

# Part 3: Interp & Evals
